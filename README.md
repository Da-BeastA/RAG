# ğŸ” LocalRAG: Retrieval-Augmented Generation (Fully Local)

LocalRAG is a fully local implementation of Retrieval-Augmented Generation (RAG) using:

- ğŸ§  [Ollama](https://ollama.com/) for local embedding and LLM inference
- ğŸ“š [Weaviate](https://weaviate.io/) as the vector database
- âš¡ Fast, secure, and completely private (no internet calls)

## ğŸš€ Features
- Text chunking + semantic embedding
- Vector-based similarity search
- Natural language query answering
- All components run on your local machine

## ğŸ›  Tech Stack
- Ollama (`llama3`, `mxbai-embed-large`)
- Weaviate (local instance)
- Python 3.10+
- UUID, requests, and Weaviate client

## ğŸ§ª Example Usage
```bash
python localrag.py

## Then enter your question interactively:
#- Ask something (or 'exit' to quit): What is this document about?

#To install the requirements:
#- pip install -r requirements.txt
